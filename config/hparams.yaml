units_type: character
feature_source: kaldi
feature_type: fbank
data_name: libriSpeech
window_size: 0.025   # s
hop_size: 0.010      # s
frame_rate: 30    # ms
preemphasis: 0.98
data_length: False # None for all data
data_path: E:\corpus_en
scp_path: D:\pycharm_proj\Speech_Transformer\datasets
shuffle: True
left_context_width: 0
right_context_width: 0
apply_cmvn: True
max_input_length: 1500 # max frames of input_data
max_target_length: 50  # max chars of input_label
num_mels: 80

model:
    name: S_Transformer
    num_M: 2
    n: 64
    c: 64
    vocab_size: 31 #  26 lowercase letters, apostrophe, period, space, noise marker and end-of-sequence tokens
    pe_max_len: 8000 # position encoding length(must be longer then max_frame_length)
    feature_dim: 320 # fbanks * (left_context_width + right_context_width + 1) == 80*4
    dropout: 0.1
    pcl: 0.8 # probability of correct label
    d_model: 256 # d_k = d_v = d_q = d_model/n_heads = 128
    n_heads: 4
    # fine-tune hyper params
    N_encoder: 6
    N_decoder: 6
    d_ff: 1024

train:
    seed: 2019
    batch_size: 16
    batch_frames: 20000
    use_gpu: True
    num_gpu: 1
    gpu_ids: 0
    max_steps: 100000 # need to be modified
    epoches: 30 # need to be specified
    label_smoothing_epsilon: 0.1
    neighborhood_smoothing: True
    visualization: True
    show_interval: 10
    save_model: logdir/logging/transformer
    dev_on_training: False

test:
    batch_size: 4

optimizer:
    warmup_n: 25000
    k: 10  # note: decreased k to 1 when the model converged
    beta1: 0.9
    beta2: 0.98
    epsilon: 1.0e-9

decode:
    beam_size: 10
    length_penalty: 1.0

debug:
    batch_size: 4